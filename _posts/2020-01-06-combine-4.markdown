---
layout: post
title:  "Combine 4 - A long overdue update"
---

> Combine is a parser combinator library for the Rust programming language

So [combine][] 4 was actually released all the way back in January and I wrote most of this post back then as well, only to lose motivation due to lack of time and a general unhappiness with the flow of this post. I finally picked this up again, rewrote the worst parts into what I hope should inspire you to give combine 4 a try!

[combine]:https://github.com/Marwes/combine

## Simplifying the internals

Compared to other parser combinator libraries, combine's internals can definitely seem intimidating. There are both more traits at play and they are also more complex. While there are good reasons for this, some improvements could still be made through some breaking changes.

Some of the internal structures and concepts have been simplified such that `combine` always talks about `Token/token` instead of `Item/item`. This does cause some fallout when upgrading as type bounds (`Stream<Item = char>`) are directly affected but consistency was deemed more important here and using `Token` differentiates `Stream` from `Iterator`.

Combine's concept of consumed/empty has been renamed to commit/peek to be more self explanatory. A parser that returns `Commit` has committed to this parse meaning parsing can't continue after failure (unless overridden with [`attempt`][]). If parser only returns `Peek` on the other hand then parsers like [`choice`][] will continue to try other parsers. (By default the behaviour is that it *commits* to a parse if it successfully *consumes* one token, hence the previous naming).

[`attempt`]:https://docs.rs/combine/*/combine/fn.attempt.html
[`choice`]:https://docs.rs/combine/*/combine/fn.choice.html

`FullRangeStream` has been absorbed into `RangeStream` as any `RangeStream` could trivially implement `FullRangeStream`.

The `Parser` trait now takes `Input` as a type parameter instead of an associated type. In theory this makes `Parser` more general as they can accept `Input` types which refer to type parameters or lifetimes which do not appear in the concrete type.

```rust
// Could write writer parsers that do not need to refer to `I` in the concrete type like this
struct Token<T> { token: T }
impl<I> Parser<I> for Token<I::Token> {
}

// Instead of this
struct Token<T, I> { token: T, _marker: PhantomData<fn (I)> }
impl<I> Parser for Token<I::Token, I> {
    type Input = I;
}
```

However in practice it turns out not to work out as well as I hoped. This is because because extensions methods on traits like [`and`][], [`and_then`][], [`message`][] etc, cause type inference issues as to the `Input` type parameter can't be inferred. Thus all parsers must still specify a phantom input type instead of leaving it to be inferred.

[`and`]:https://docs.rs/combine/*/combine/trait.Parser.html#method.and
[`and_then`]:https://docs.rs/combine/*/combine/trait.Parser.html#method.and_then
[`message`]:https://docs.rs/combine/*/combine/trait.Parser.html#method.message

Taking a type parameter still seems worthwhile though as it is more correct in the sense that type parameters are input types, and it could still open up for parsers being more generic in the future so better to do the breaking change now than later.

## Higher ranked parser types (but not really)



While it is impossible to define helper functions to parse `Read`, it is actually possible to define macros that, with some limitations, allows for simple parsing from io sources without reading the entire thing into memory, without re-parsing parts of the input after more data was found to be needed, while still efficiently parsing plain `&[u8]` slices(*).

[`decode!`][], [`decode_futures_03!`][] and [`decode_tokio_02!`][] each implements this decoding scheme on `std::io::Read`, `futures::io::AsyncRead` and `tokio::io::AsyncRead`. The macro and associated `Decoder` struct handles reading and buffering the data that the parser requires so that the parser itself only needs to worry about how much data it needs at most to complete a parse, but not how to efficiently and correctly buffer it.


```rust
use futures::pin_mut;
use tokio::{
    fs::File,
};

use combine::{decode_tokio_02, satisfy, skip_many1, many1, sep_end_by, Parser, stream::Decoder};

#[tokio::main]
async fn main() {
    let mut read = File::open("README.md").await.unwrap();
    let mut decoder = Decoder::new();
    let is_whitespace = |b: u8| b == b' ' || b == b'\r' || b == b'\n';
    assert_eq!(
        decode_tokio_02!(
            decoder,
            &mut read,
            {
                let word = many1(satisfy(|b| !is_whitespace(b)));
                sep_end_by(word, skip_many1(satisfy(is_whitespace))).map(|words: Vec<Vec<u8>>| words.len())
            },
            |input, _position| combine::easy::Stream::from(input),
        ).map_err(combine::easy::Errors::<u8, &[u8], _>::from),
        Ok(819),
    );
}
```

[`decode!`]:https://docs.rs/combine/*/combine/macro.decode.html
[`decode_futures_03!`]:https://docs.rs/combine/*/combine/macro.decode_futures_03.html
[`decode_tokio_02!`]:https://docs.rs/combine/*/combine/macro.decode_tokio_02.html


(*) `combine::stream::read::Stream` could and can still be used to parse directly from a `std::io::Read`, but it comes with a bit more overhead to to the extra complexity when fetching the next byte and it can't use combine's [`range`][] parsers which require slices to work.

[`range`]:https://docs.rs/combine/*/combine/parser/range/index.html


## More fixes

`PartialEq` is no longer required on `Token` (`Item`) and `Range`. These bounds shouldn't have been needed after 3.0 revamped the error implementation but since they were forgotten I had to wait until a breaking release to remove them.

[`reset`][] can now return an error, allowing `BufferedStream` to correctly error in more cases where it's buffer is exceeded.

[`reset`]:https://docs.rs/combine/*/combine/stream/trait.ResetStream.html#tymethod.reset

## Added parsers

A few more parsers have been added, the most generally useful probably being [`dispatch!`][] which more or less is a `match` expression which works with `parsers`.

```rust
use combine::{dispatch, any, token, satisfy, EasyParser, Parser};

let mut parser = any().then(|e| {
    dispatch!(e;
        'a' => token('a'),
        'b' => satisfy(|b| b == 'b'),
        t if t == 'c' => any(),
        _ => token('d')
    )
});
assert_eq!(parser.easy_parse("aa"), Ok(('a', "")));
assert_eq!(parser.easy_parse("cc"), Ok(('c', "")));
assert_eq!(parser.easy_parse("cd"), Ok(('d', "")));
assert!(parser.easy_parse("ab").is_err());
```

Other added parsers are [`repeat_until`][], [`iterate`][], [`produce`][] and [`factory`][]

[`dispatch!`]:https://docs.rs/combine/*/combine/macro.dispatch.html
[`repeat_until`]:https://docs.rs/combine/*/combine/parser/repeat/fn.repeat_until.html
[`iterate`]:https://docs.rs/combine/*/combine/parser/repeat/fn.iterate.html
[`produce`]:https://docs.rs/combine/*/combine/fn.produce.html
[`factory`]:https://docs.rs/combine/*/combine/parser/combinator/fn.factory.html


## More examples

`combine` is usually a crate I leave alone until I have some ideas for using it or there is an issue reported on it. This release and the additions in it is no different and I managed to work on two small projects which were in need for parsing while making the changes for 4.0.

### combine_jpeg

[combine_jpeg][] is a basic jpeg decoder which really tests the limits of combine's partial parsing. Instead of the complicated state storing and loading of https://github.com/mozilla/mozjpeg or being tied to synchronous file reading like in https://github.com/kaksmet/jpeg-decoder I wanted to try for the speed of mozjpeg without the complexity (or at least while hiding the complexity in combine).

The current implementation falls short of that ambition however, it is still quite complex as some trickery is needed to translate between the byte and bit based streams used in the jpeg format and I haven't been able to reach the performance of mozjpeg. The performance is still better than (single threaded) https://github.com/kaksmet/jpeg-decoder on the images that combine_jpeg can decode but without figuring out where the remaining gap comes from I don't feel motivated to extend the project further.

[combine_jpeg]:https://github.com/Marwes/combine-jpeg

### kafka_protocol

[kafka_protocol][] implements encoding and decoding of [Kafka][] reuqests and responses where combine handles the decoding part. Since all kafka messages are length prefixed there isn't really a pressing need for combine so far as it is always known how much need to be read into memory before processing a request/response. For large fetch/produce calls it might be useful to incrementally decode messages to reduce memory at some later point however so it might come in play there.

In any case it serves as a good display for how binary formats can be handled with combine. All protocol types are actually generated from the protocol specification (which again, is parsed with combine) letting new versions the protocol be created easily.

While sending and receiving requests works well in my testing, there is a lot more to a kafka client so think of this more as a building block to an eventual kafka client.

[kafka_protocol]:https://github.com/Marwes/combine-jpeg
[Kafka]:https://kafka.apache.org/
